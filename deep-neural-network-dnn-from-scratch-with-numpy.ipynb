{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1703e63a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-02T05:42:17.330221Z",
     "iopub.status.busy": "2023-08-02T05:42:17.329754Z",
     "iopub.status.idle": "2023-08-02T05:42:23.278008Z",
     "shell.execute_reply": "2023-08-02T05:42:23.276513Z"
    },
    "papermill": {
     "duration": 5.957558,
     "end_time": "2023-08-02T05:42:23.282353",
     "exception": false,
     "start_time": "2023-08-02T05:42:17.324795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.9475\n",
      "Epoch 2/30, Loss: 2.6365\n",
      "Epoch 3/30, Loss: 2.4741\n",
      "Epoch 4/30, Loss: 2.3779\n",
      "Epoch 5/30, Loss: 2.3144\n",
      "Epoch 6/30, Loss: 2.2685\n",
      "Epoch 7/30, Loss: 2.2284\n",
      "Epoch 8/30, Loss: 2.1967\n",
      "Epoch 9/30, Loss: 2.1673\n",
      "Epoch 10/30, Loss: 2.1415\n",
      "Epoch 11/30, Loss: 2.1195\n",
      "Epoch 12/30, Loss: 2.0996\n",
      "Epoch 13/30, Loss: 2.0762\n",
      "Epoch 14/30, Loss: 2.0575\n",
      "Epoch 15/30, Loss: 2.0319\n",
      "Epoch 16/30, Loss: 2.0038\n",
      "Epoch 17/30, Loss: 1.9796\n",
      "Epoch 18/30, Loss: 1.9551\n",
      "Epoch 19/30, Loss: 1.9272\n",
      "Epoch 20/30, Loss: 1.8935\n",
      "Epoch 21/30, Loss: 1.8625\n",
      "Epoch 22/30, Loss: 1.8293\n",
      "Epoch 23/30, Loss: 1.8013\n",
      "Epoch 24/30, Loss: 1.7667\n",
      "Epoch 25/30, Loss: 1.7260\n",
      "Epoch 26/30, Loss: 1.6854\n",
      "Epoch 27/30, Loss: 1.6473\n",
      "Epoch 28/30, Loss: 1.6121\n",
      "Epoch 29/30, Loss: 1.5785\n",
      "Epoch 30/30, Loss: 1.5502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Softmax activation function with max subtraction for numerical stability\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Categorical cross-entropy loss function\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# BatchNormalization layer class\n",
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim):\n",
    "        self.gamma = np.ones(input_dim)\n",
    "        self.beta = np.zeros(input_dim)\n",
    "        self.epsilon = 1e-8\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.input = None\n",
    "        self.normalized = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.mean = np.mean(input_data, axis=0)\n",
    "        self.var = np.var(input_data, axis=0)\n",
    "        self.normalized = (input_data - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "        return self.gamma * self.normalized + self.beta\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        batch_size = d_output.shape[0]\n",
    "        d_normalized = d_output * self.gamma\n",
    "        d_var = np.sum(d_normalized * (self.input - self.mean) * -0.5 * (self.var + self.epsilon) ** (-1.5), axis=0)\n",
    "        d_mean = np.sum(d_normalized * -1 / np.sqrt(self.var + self.epsilon), axis=0) + d_var * np.mean(-2 * (self.input - self.mean), axis=0)\n",
    "        d_input = d_normalized / np.sqrt(self.var + self.epsilon) + d_var * 2 * (self.input - self.mean) / batch_size + d_mean / batch_size\n",
    "        d_gamma = np.sum(d_output * self.normalized, axis=0)\n",
    "        d_beta = np.sum(d_output, axis=0)\n",
    "        return d_input, d_gamma, d_beta\n",
    "\n",
    "# Dense layer class\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2 / input_dim)\n",
    "        self.bias = np.zeros(output_dim)\n",
    "        self.activation = activation\n",
    "        self.input = None\n",
    "        self.z = None\n",
    "        self.batch_norm = BatchNormalization(output_dim)  # Add BatchNormalization\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.z = np.dot(input_data, self.weights) + self.bias\n",
    "        normalized = self.batch_norm.forward(self.z)  # Use BatchNormalization\n",
    "        return self.activation(normalized)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_activation = np.where(self.z > 0, 1, 0)\n",
    "        d_z = d_output * d_activation\n",
    "        d_input, d_weights, d_bias = self.batch_norm.backward(d_z)  # Use BatchNormalization\n",
    "        d_input = np.dot(d_input, self.weights.T)\n",
    "        d_weights = np.dot(self.input.T, d_z)\n",
    "        d_bias = np.sum(d_z, axis=0)\n",
    "        return d_input, d_weights, d_bias\n",
    "\n",
    "# Model class\n",
    "class DNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_output, d_weights, d_bias = layer.backward(d_output)\n",
    "            # Update weights and biases using the optimizer\n",
    "            layer.weights -= self.optimizer * d_weights\n",
    "            layer.bias -= self.optimizer * d_bias\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train[i:i+batch_size]\n",
    "                batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(batch_X)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss(batch_y, predictions)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backward pass\n",
    "                d_output = predictions - batch_y  # Fixed the backward pass\n",
    "                self.backward(d_output)\n",
    "\n",
    "            epoch_loss /= len(X_train)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Generating some dummy data to test the model\n",
    "X_train = np.random.randn(1000, 28, 28)\n",
    "y_train = np.random.randint(0, 10, (1000, 10))\n",
    "y_train = (y_train == np.arange(10)).astype(float)\n",
    "\n",
    "# Creating and training the model\n",
    "model = DNN()\n",
    "model.add_layer(DenseLayer(input_dim=28*28, output_dim=256, activation=relu))\n",
    "model.add_layer(DenseLayer(input_dim=256, output_dim=128, activation=relu))\n",
    "model.add_layer(DenseLayer(input_dim=128, output_dim=10, activation=softmax))\n",
    "model.compile(loss=categorical_crossentropy, optimizer=0.001)\n",
    "model.fit(X_train.reshape(-1, 28*28), y_train, epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772315f2",
   "metadata": {
    "papermill": {
     "duration": 0.007375,
     "end_time": "2023-08-02T05:42:23.298124",
     "exception": false,
     "start_time": "2023-08-02T05:42:23.290749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Schema for the Deep Neural Network (DNN):**\n",
    "\n",
    "```\n",
    "Input Data (X_train) [Batch Size x Input Size]\n",
    " |\n",
    " V\n",
    "[Dense Layer 1]\n",
    "   |\n",
    "   V\n",
    "[Batch Normalization 1]\n",
    "   |\n",
    "   V\n",
    "[ReLU Activation 1]\n",
    "   |\n",
    "   V\n",
    "[Dense Layer 2]\n",
    "   |\n",
    "   V\n",
    "[Batch Normalization 2]\n",
    "   |\n",
    "   V\n",
    "[ReLU Activation 2]\n",
    "   |\n",
    "   V\n",
    "[Dense Layer 3]\n",
    "   |\n",
    "   V\n",
    "[Softmax Activation]\n",
    "   |\n",
    "   V\n",
    "Predicted Probabilities [Batch Size x Output Size]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **Input Data (X_train):** The input data consists of a batch of samples, where each sample represents a flattened 28x28 image (784-dimensional vector). The size of the batch is determined by the chosen batch size, and each sample is fed into the DNN for training.\n",
    "\n",
    "2. **Dense Layer 1:** The first dense layer receives the input data, which is a 784-dimensional vector for each sample. The layer has 256 neurons, and each neuron is fully connected to the input. The neurons in this layer learn to extract features from the input data.\n",
    "\n",
    "3. **Batch Normalization 1:** After the dense layer, batch normalization is applied to normalize the output of the layer. It helps stabilize training by reducing internal covariate shift and speeding up convergence.\n",
    "\n",
    "4. **ReLU Activation 1:** The ReLU (Rectified Linear Unit) activation function is applied after batch normalization. ReLU introduces non-linearity to the model, allowing it to learn complex patterns and making the training process more efficient.\n",
    "\n",
    "5. **Dense Layer 2:** The second dense layer receives the output from ReLU Activation 1. It has 128 neurons, and each neuron is fully connected to the ReLU activation output. This layer further learns higher-level features from the input.\n",
    "\n",
    "6. **Batch Normalization 2:** Similar to Batch Normalization 1, batch normalization is applied to the output of Dense Layer 2 to improve training stability.\n",
    "\n",
    "7. **ReLU Activation 2:** ReLU is applied after Batch Normalization 2 to introduce non-linearity and enhance the model's representational power.\n",
    "\n",
    "8. **Dense Layer 3:** The third dense layer receives the output from ReLU Activation 2. It has 10 neurons, each representing one digit class (0 to 9). This is the final layer before the softmax activation.\n",
    "\n",
    "9. **Softmax Activation:** The softmax activation function is applied after Dense Layer 3. It converts the raw scores of the neurons into probabilities, representing the model's prediction of each input sample belonging to each digit class.\n",
    "\n",
    "10. **Predicted Probabilities:** The final output of the DNN is a matrix of predicted probabilities, where each row corresponds to a sample in the batch, and each column represents the predicted probability for one of the 10 digit classes.\n",
    "\n",
    "During the training process, the model adjusts the weights and biases of each dense layer to minimize the categorical cross-entropy loss between the predicted probabilities and the actual one-hot encoded labels (y_train). The optimization is performed using the Adam optimizer with a learning rate of 0.001. The training proceeds through 35 epochs, and the loss is monitored at each epoch to track the model's performance and convergence.\n",
    "\n",
    "By training this DNN on a dataset of images and corresponding labels, the model can learn to recognize and classify the digits in the images accurately. This is a simplified version of a DNN, and in practice, more complex architectures and techniques are used to achieve better performance on real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85611e5",
   "metadata": {
    "papermill": {
     "duration": 0.007235,
     "end_time": "2023-08-02T05:42:23.313238",
     "exception": false,
     "start_time": "2023-08-02T05:42:23.306003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.665377,
   "end_time": "2023-08-02T05:42:24.254014",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-02T05:42:05.588637",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
